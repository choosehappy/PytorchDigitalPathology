{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#v3.classification\n",
    "#17/8/2019\n",
    "#modified augmentation approach to use albumentations: \n",
    "#https://github.com/albu/albumentations \n",
    "#https://albumentations.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "dataname=\"lymphoma\"\n",
    "gpuid=0\n",
    "\n",
    "# --- densenet params\n",
    "#these parameters get fed directly into the densenet class, and more description of them can be discovered there\n",
    "num_classes=3    #number of classes in the data mask that we'll aim to predict\n",
    "in_channels= 3  #input channel of the data, RGB = 3\n",
    "\n",
    "\n",
    "growth_rate=32 \n",
    "block_config=(2, 2, 2, 2)\n",
    "num_init_features=64\n",
    "bn_size=4\n",
    "drop_rate=0\n",
    "\n",
    "\n",
    "\n",
    "# --- training params\n",
    "batch_size=128\n",
    "patch_size=224 #currently, this needs to be 224 due to densenet architecture\n",
    "num_epochs = 100\n",
    "phases = [\"train\",\"val\"] #how many phases did we create databases for?\n",
    "validation_phases= [\"val\"] #when should we do valiation? note that validation is *very* time consuming, so as opposed to doing for both training and validation, we do it only for vlaidation at the end of the epoch\n",
    "                           #additionally, using simply [], will skip validation entirely, drastically speeding things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import DenseNet\n",
    "\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import sys, glob\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import time\n",
    "import math\n",
    "import tables\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for pretty printing of current time and remaining time\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent+.00001)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify if we should use a GPU (cuda) or only the CPU\n",
    "print(torch.cuda.get_device_properties(gpuid))\n",
    "torch.cuda.set_device(gpuid)\n",
    "device = torch.device(f'cuda:{gpuid}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build the model according to the paramters specified above and copy it to the GPU. finally print out the number of trainable parameters\n",
    " \n",
    "model = DenseNet(growth_rate=growth_rate, block_config=block_config,\n",
    "                 num_init_features=num_init_features, \n",
    "                 bn_size=bn_size, \n",
    "                 drop_rate=drop_rate, \n",
    "                 num_classes=num_classes).to(device)\n",
    "#model = DenseNet(growth_rate=32, block_config=(6, 12, 24, 16), #these represent the default parameters\n",
    "#                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=3)\n",
    "\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this defines our dataset class which will be used by the dataloader\n",
    "class Dataset(object):\n",
    "    def __init__(self, fname ,img_transform=None):\n",
    "        #nothing special here, just internalizing the constructor parameters\n",
    "        self.fname=fname\n",
    "\n",
    "        self.img_transform=img_transform\n",
    "        \n",
    "        with tables.open_file(self.fname,'r') as db:\n",
    "            self.classsizes=db.root.classsizes[:]\n",
    "            self.nitems=db.root.imgs.shape[0]\n",
    "        \n",
    "        self.imgs = None\n",
    "        self.labels = None\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #opening should be done in __init__ but seems to be\n",
    "        #an issue with multithreading so doing here. need to do it everytime, otherwise hdf5 crashes\n",
    "\n",
    "        with tables.open_file(self.fname,'r') as db:\n",
    "            self.imgs=db.root.imgs\n",
    "            self.labels=db.root.labels\n",
    "\n",
    "            #get the requested image and mask from the pytable\n",
    "            img = self.imgs[index,:,:,:]\n",
    "            label = self.labels[index]\n",
    "        \n",
    "        \n",
    "        img_new = img\n",
    "        \n",
    "        if self.img_transform:\n",
    "            img_new = self.img_transform(image=img)['image']\n",
    "\n",
    "        return img_new, label, img\n",
    "    def __len__(self):\n",
    "        return self.nitems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#https://github.com/albu/albumentations/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb\n",
    "img_transform = Compose([\n",
    "       VerticalFlip(p=.5),\n",
    "       HorizontalFlip(p=.5),\n",
    "       HueSaturationValue(hue_shift_limit=(-25,0),sat_shift_limit=0,val_shift_limit=0,p=1),\n",
    "       Rotate(p=1, border_mode=cv2.BORDER_CONSTANT,value=0),\n",
    "       #ElasticTransform(always_apply=True, approximate=True, alpha=150, sigma=8,alpha_affine=50),\n",
    "       RandomSizedCrop((patch_size,patch_size), patch_size,patch_size),\n",
    "       ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "dataset={}\n",
    "dataLoader={}\n",
    "for phase in phases: #now for each of the phases, we're creating the dataloader\n",
    "                     #interestingly, given the batch size, i've not seen any improvements from using a num_workers>0\n",
    "    \n",
    "    dataset[phase]=Dataset(f\"./{dataname}_{phase}.pytable\", img_transform=img_transform)\n",
    "    dataLoader[phase]=DataLoader(dataset[phase], batch_size=batch_size, \n",
    "                                shuffle=True, num_workers=8,pin_memory=True) \n",
    "    print(f\"{phase} dataset size:\\t{len(dataset[phase])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize a single example to verify that it is correct\n",
    "(img, label, img_old)=dataset[\"train\"][7]\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))  # 1 row, 2 columns\n",
    "\n",
    "#build output showing patch after augmentation and original patch\n",
    "ax[0].imshow(np.moveaxis(img.numpy(),0,-1))\n",
    "ax[1].imshow(img_old)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters()) #adam is going to be the most robust, though perhaps not the best performing, typically a good place to start\n",
    "# optim = torch.optim.SGD(model.parameters(),\n",
    "#                           lr=.1,\n",
    "#                           momentum=0.9,\n",
    "#                           weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#we have the ability to weight individual classes, in this case we'll do so based on their presense in the trainingset\n",
    "#to avoid biasing any particular class\n",
    "nclasses = dataset[\"train\"].classsizes.shape[0]\n",
    "class_weight=dataset[\"train\"].classsizes\n",
    "class_weight = torch.from_numpy(1-class_weight/class_weight.sum()).type('torch.FloatTensor').to(device)\n",
    "\n",
    "print(class_weight) #show final used weights, make sure that they're reasonable before continouing\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#def trainnetwork():\n",
    "writer=SummaryWriter() #open the tensorboard visualiser\n",
    "best_loss_on_test = np.Infinity\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    #zero out epoch based performance variables \n",
    "    all_acc = {key: 0 for key in phases} \n",
    "    all_loss = {key: torch.zeros(0).to(device) for key in phases} #keep this on GPU for greatly improved performance\n",
    "    cmatrix = {key: np.zeros((num_classes,num_classes)) for key in phases}\n",
    "\n",
    "    for phase in phases: #iterate through both training and validation states\n",
    "\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else: #when in eval mode, we don't want parameters to be updated\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        for ii , (X, label, img_orig) in enumerate(dataLoader[phase]): #for each of the batches\n",
    "            X = X.to(device)  # [Nbatch, 3, H, W]\n",
    "            label = label.type('torch.LongTensor').to(device)  # [Nbatch, 1] with class indices (0, 1, 2,...num_classes)\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'): #dynamically set gradient computation, in case of validation, this isn't needed\n",
    "                                                            #disabling is good practice and improves inference time\n",
    "\n",
    "                prediction = model(X)  # [N, Nclass]\n",
    "                loss = criterion(prediction, label)\n",
    "\n",
    "\n",
    "                if phase==\"train\": #in case we're in train mode, need to do back propogation\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    train_loss = loss\n",
    "\n",
    "\n",
    "                all_loss[phase]=torch.cat((all_loss[phase],loss.detach().view(1,-1)))\n",
    "\n",
    "                if phase in validation_phases: #if this phase is part of validation, compute confusion matrix\n",
    "                    p=prediction.detach().cpu().numpy()\n",
    "                    cpredflat=np.argmax(p,axis=1).flatten()\n",
    "                    yflat=label.cpu().numpy().flatten()\n",
    "\n",
    "                    cmatrix[phase]=cmatrix[phase]+confusion_matrix(yflat,cpredflat, labels=range(nclasses))\n",
    "\n",
    "        all_acc[phase]=(cmatrix[phase]/cmatrix[phase].sum()).trace()\n",
    "        all_loss[phase] = all_loss[phase].cpu().numpy().mean()\n",
    "\n",
    "        #save metrics to tensorboard\n",
    "        writer.add_scalar(f'{phase}/loss', all_loss[phase], epoch)\n",
    "        if phase in validation_phases:\n",
    "            writer.add_scalar(f'{phase}/acc', all_acc[phase], epoch)\n",
    "            for r in range(nclasses):\n",
    "                for c in range(nclasses): #essentially write out confusion matrix\n",
    "                    writer.add_scalar(f'{phase}/{r}{c}', cmatrix[phase][r][c],epoch)\n",
    "\n",
    "    print('%s ([%d/%d] %d%%), train loss: %.4f test loss: %.4f' % (timeSince(start_time, (epoch+1) / num_epochs), \n",
    "                                                 epoch+1, num_epochs ,(epoch+1) / num_epochs * 100, all_loss[\"train\"], all_loss[\"val\"]),end=\"\")    \n",
    "\n",
    "    #if current loss is the best we've seen, save model state with all variables\n",
    "    #necessary for recreation\n",
    "    if all_loss[\"val\"] < best_loss_on_test:\n",
    "        best_loss_on_test = all_loss[\"val\"]\n",
    "        print(\"  **\")\n",
    "        state = {'epoch': epoch + 1,\n",
    "         'model_dict': model.state_dict(),\n",
    "         'optim_dict': optim.state_dict(),\n",
    "         'best_loss_on_test': all_loss,\n",
    "         'in_channels': in_channels,\n",
    "         'growth_rate':growth_rate,\n",
    "         'block_config':block_config,\n",
    "         'num_init_features':num_init_features,\n",
    "         'bn_size':bn_size,\n",
    "         'drop_rate':drop_rate,\n",
    "         'num_classes':num_classes}\n",
    "\n",
    "\n",
    "        torch.save(state, f\"{dataname}_densenet_best_model.pth\")\n",
    "    else:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "#%lprun -f trainnetwork trainnetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this stage, training is done...below are snippets to help with other tasks: output generation + visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- generate output\n",
    "#load best model\n",
    "checkpoint = torch.load(f\"{dataname}_densenet_best_model.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab a single image from validation set\n",
    "(img, label, img_old)=dataset[\"val\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate its output\n",
    "#%%timeit\n",
    "output=model(img[None,::].to(device))\n",
    "output=output.detach().squeeze().cpu().numpy()\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)\n",
    "print(f\"True class:{label}\")\n",
    "print(f\"Predicted class:{np.argmax(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at input\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))  # 1 row, 2 columns\n",
    "\n",
    "ax[0].imshow(np.moveaxis(img.numpy(),0,-1))\n",
    "ax[1].imshow(img_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- visualize kernels and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for visualization\n",
    "def plot_kernels(tensor, num_cols=8 ,cmap=\"gray\"):\n",
    "    if not len(tensor.shape)==4:\n",
    "        raise Exception(\"assumes a 4D tensor\")\n",
    "#    if not tensor.shape[1]==3:\n",
    "#        raise Exception(\"last dim needs to be 3 to plot\")\n",
    "    num_kernels = tensor.shape[0] * tensor.shape[1]\n",
    "    num_rows = 1+ num_kernels // num_cols\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    i=0\n",
    "    t=tensor.data.numpy()\n",
    "    for t1 in t:\n",
    "        for t2 in t1:\n",
    "            i+=1\n",
    "            ax1 = fig.add_subplot(num_rows,num_cols,i)\n",
    "            ax1.imshow(t2 , cmap=cmap)\n",
    "            ax1.axis('off')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_yticklabels([])\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerActivations():\n",
    "    features=None\n",
    "    def __init__(self,layer):\n",
    "        self.hook = layer.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self,module,input,output):\n",
    "        self.features = output.cpu()\n",
    "    def remove(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- visualize kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=model.features.denseblock2.denselayer1.conv2\n",
    "plot_kernels(w.weight.detach().cpu()[0:5,0:5,:,:],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- visualize activiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr=LayerActivations(model.features.denseblock2.denselayer1.conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(img, label, img_old)=dataset[\"val\"][7]\n",
    "plt.imshow(np.moveaxis(img.numpy(),0,-1))\n",
    "output=model(img[None,::].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernels(dr.features,8,cmap=\"rainbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# ---- Improvements:\n",
    "1 replace Adam with SGD with appropriate learning rate reduction"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
